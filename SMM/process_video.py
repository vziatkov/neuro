#!/usr/bin/env python3
"""
Neuro Video Processor
=====================
–°–æ–∑–¥–∞—ë—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ-—Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ –≤ —Å—Ç–∏–ª–µ "Neuro: –ñ–∏–≤–æ–µ –∑–µ—Ä–∫–∞–ª–æ —Å–æ–∑–Ω–∞–Ω–∏—è"

–≠—Ñ—Ñ–µ–∫—Ç—ã:
- Bloom/Glow (–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ)
- Breath rhythm (—Ü–∏–∫–ª –¥—ã—Ö–∞–Ω–∏—è)
- Photonic noise (–∫–≤–∞–Ω—Ç–æ–≤–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å)
- Color shift (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏)
- Edge enhancement (–≥—Ä–∞–Ω–∏—Ü—ã —Å–æ–∑–Ω–∞–Ω–∏—è)
"""

import os
import sys
import numpy as np
from pathlib import Path
from typing import Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

try:
    from moviepy.editor import VideoFileClip, ImageSequenceClip, CompositeVideoClip, AudioFileClip, concatenate_videoclips
    from moviepy.video.fx.all import fadein, fadeout
    import cv2
    from PIL import Image, ImageFilter, ImageEnhance
    from scipy import ndimage
    from scipy.signal import butter, filtfilt
except ImportError as e:
    print(f"‚ùå Missing dependency: {e}")
    print("üì¶ Install: pip install moviepy opencv-python numpy pillow scipy")
    sys.exit(1)


# ============================================================================
# –¶–í–ï–¢–û–í–ê–Ø –ü–ê–õ–ò–¢–†–ê NEURO (–≥–ª—É–±–æ–∫–∏–µ —Å–∏–Ω–∏–µ, —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–µ, –ª–∞–∑—É—Ä–Ω—ã–µ)
# ============================================================================
NEURO_PALETTE = {
    'deep_night': np.array([0x03, 0x0b, 0x18]),  # #030b18
    'violet': np.array([0x7d, 0x5b, 0xff]),      # #7d5bff
    'azure': np.array([0x4f, 0xd1, 0xc5]),       # #4fd1c5
    'purple': np.array([0x8b, 0x5c, 0xf6]),      # #8b5cf6
    'dark_blue': np.array([0x1e, 0x3a, 0x8a]),   # #1e3a8a
}


# ============================================================================
# –§–ò–õ–¨–¢–†–´ –ò –≠–§–§–ï–ö–¢–´
# ============================================================================

def glow_filter(frame: np.ndarray, intensity: float = 0.3) -> np.ndarray:
    """
    –£—Å–∏–ª–∏–≤–∞–µ—Ç —Å–≤–µ—Ç, —Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É—è –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ.
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç bloom-—ç—Ñ—Ñ–µ–∫—Ç —á–µ—Ä–µ–∑ —Ä–∞–∑–º—ã—Ç–∏–µ –∏ –∞–¥–¥–∏—Ç–∏–≤–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ.
    """
    # Gaussian blur –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–µ—á–µ–Ω–∏—è
    blurred = cv2.GaussianBlur(frame, (0, 0), sigmaX=intensity * 10, sigmaY=intensity * 10)
    # –ê–¥–¥–∏—Ç–∏–≤–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Å–≤–µ—Ç–∞
    glow = cv2.addWeighted(frame, 1.0, blurred, intensity, 0)
    return np.clip(glow, 0, 255).astype(np.uint8)


def edge_enhancement(frame: np.ndarray, strength: float = 0.2) -> np.ndarray:
    """
    –ü–æ–¥—á—ë—Ä–∫–∏–≤–∞–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã –º–µ–∂–¥—É –º—ã—Å–ª—å—é –∏ –º–∞—Ç–µ—Ä–∏–µ–π.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Canny edge detection —Å –º—è–≥–∫–∏–º –Ω–∞–ª–æ–∂–µ–Ω–∏–µ–º.
    """
    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
    edges = cv2.Canny(gray, 50, 150)
    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
    # –ú—è–≥–∫–æ–µ –Ω–∞–ª–æ–∂–µ–Ω–∏–µ –∫—Ä–∞—ë–≤
    enhanced = cv2.addWeighted(frame, 1.0 - strength, edges_colored, strength, 0)
    return np.clip(enhanced, 0, 255).astype(np.uint8)


def color_shift(frame: np.ndarray, t: float) -> np.ndarray:
    """
    –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–æ–∑–Ω–∞–Ω–∏—è.
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—É—é —Ü–≤–µ—Ç–æ–≤—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –Ω–µ–π—Ä–æ–Ω–Ω—É—é –ø–∞–ª–∏—Ç—Ä—É.
    """
    h, w = frame.shape[:2]
    
    # –°–æ–∑–¥–∞—ë–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –æ—Ç –≥–ª—É–±–æ–∫–æ–π –Ω–æ—á–∏ –∫ —Ñ–∏–æ–ª–µ—Ç–æ–≤–æ–º—É
    gradient = np.linspace(0, 1, w)
    gradient = np.tile(gradient, (h, 1))
    
    # –ü—É–ª—å—Å–∏—Ä—É—é—â–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    pulse = 0.5 + 0.3 * np.sin(t * 0.5)
    
    result = frame.copy().astype(np.float32)
    
    # –°–º–µ—à–∏–≤–∞–µ–º —Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π –ø–∞–ª–∏—Ç—Ä–æ–π
    for i in range(3):
        night_val = NEURO_PALETTE['deep_night'][i]
        violet_val = NEURO_PALETTE['violet'][i]
        channel_gradient = night_val + (violet_val - night_val) * gradient
        result[:, :, i] = result[:, :, i] * (1 - pulse * 0.4) + channel_gradient * (pulse * 0.4)
    
    return np.clip(result, 0, 255).astype(np.uint8)


def photonic_noise(frame: np.ndarray, t: float, density: float = 0.01) -> np.ndarray:
    """
    –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–≤–∞–Ω—Ç–æ–≤—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –∏ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å.
    –î–æ–±–∞–≤–ª—è–µ—Ç —Å–≤–µ—Ç—è—â–∏–µ—Å—è —Ç–æ—á–∫–∏, –¥–≤–∏–∂—É—â–∏–µ—Å—è –ø–æ —Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É.
    """
    h, w = frame.shape[:2]
    result = frame.copy().astype(np.float32)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ—á–∫–∏ —Å —Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω—ã–º –¥–≤–∏–∂–µ–Ω–∏–µ–º
    num_points = int(h * w * density)
    
    for _ in range(num_points):
        x = int(np.random.uniform(0, w))
        y = int(np.random.uniform(0, h))
        
        # –°–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ
        offset_x = int(5 * np.sin(t * 2 + y * 0.01))
        offset_y = int(5 * np.cos(t * 2 + x * 0.01))
        px = (x + offset_x) % w
        py = (y + offset_y) % h
        
        # –°–≤–µ—Ç—è—â–∞—è—Å—è —Ç–æ—á–∫–∞ (–ª–∞–∑—É—Ä–Ω—ã–π —Ü–≤–µ—Ç)
        if 0 <= px < w and 0 <= py < h:
            result[py, px, :] = np.minimum(result[py, px, :] + NEURO_PALETTE['azure'] * 0.3, 255)
    
    return np.clip(result, 0, 255).astype(np.uint8)


def breath_rhythm(frame: np.ndarray, t: float, period: float = 6.0) -> np.ndarray:
    """
    –ù–∞–ø–æ–º–∏–Ω–∞–µ—Ç –æ —Ü–∏–∫–ª–µ –≤–¥–æ—Ö–∞-–≤—ã–¥–æ—Ö–∞ –∫–∞–∫ –æ—Å–Ω–æ–≤–µ –∂–∏–∑–Ω–∏.
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—É–ª—å—Å–∏—Ä—É—é—â–µ–µ —Ä–∞–∑–º—ã—Ç–∏–µ –∏ —è—Ä–∫–æ—Å—Ç—å –ø–æ —Ä–∏—Ç–º—É –¥—ã—Ö–∞–Ω–∏—è.
    """
    # Ease-in-out —Å–∏–Ω—É—Å–æ–∏–¥–∞ –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –¥—ã—Ö–∞–Ω–∏—è
    phase = 2 * np.pi * t / period
    breath = 0.5 + 0.3 * np.sin(phase)
    
    # –ü–ª–∞–≤–Ω—ã–π ease-in-out
    breath = np.sin((phase + np.pi/2) / 2) * 0.6 + 0.5
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–º—ã—Ç–∏–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥—ã—Ö–∞–Ω–∏—é
    blur_amount = int(breath * 2)
    if blur_amount > 0:
        blurred = cv2.GaussianBlur(frame, (0, 0), sigmaX=blur_amount, sigmaY=blur_amount)
        # –°–º–µ—à–∏–≤–∞–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º
        result = cv2.addWeighted(frame, 1.0 - breath * 0.3, blurred, breath * 0.3, 0)
    else:
        result = frame
    
    # –ü—É–ª—å—Å–∏—Ä—É—é—â–∞—è —è—Ä–∫–æ—Å—Ç—å
    brightness = 1.0 + breath * 0.1
    result = np.clip(result * brightness, 0, 255).astype(np.uint8)
    
    return result


# ============================================================================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –û–ë–†–ê–ë–û–¢–ö–ò –ö–ê–î–†–ê
# ============================================================================

def process_frame(frame: np.ndarray, t: float) -> np.ndarray:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤—Å–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –∫ –æ–¥–Ω–æ–º—É –∫–∞–¥—Ä—É.
    –ü–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.
    """
    # 1. –¶–≤–µ—Ç–æ–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è (–±–∞–∑–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞)
    frame = color_shift(frame, t)
    
    # 2. –£—Å–∏–ª–µ–Ω–∏–µ –∫—Ä–∞—ë–≤ (–≥—Ä–∞–Ω–∏—Ü—ã —Å–æ–∑–Ω–∞–Ω–∏—è)
    frame = edge_enhancement(frame, strength=0.15)
    
    # 3. –°–≤–µ—á–µ–Ω–∏–µ (–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ)
    frame = glow_filter(frame, intensity=0.25)
    
    # 4. –†–∏—Ç–º –¥—ã—Ö–∞–Ω–∏—è (–∂–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª)
    frame = breath_rhythm(frame, t, period=6.0)
    
    # 5. –§–æ—Ç–æ–Ω–Ω—ã–π —à—É–º (–∫–≤–∞–Ω—Ç–æ–≤–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å)
    frame = photonic_noise(frame, t, density=0.008)
    
    return frame


# ============================================================================
# –ì–ï–ù–ï–†–ê–¶–ò–Ø –ê–£–î–ò–û
# ============================================================================

def generate_breath_audio(duration: float, fps: int = 44100) -> np.ndarray:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∏–∑–∫–∏–π –≥—É–ª –∏–ª–∏ heartbeat-loop –¥–ª—è –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã.
    """
    t = np.linspace(0, duration, int(fps * duration))
    
    # –ù–∏–∑–∫–∏–π –≥—É–ª (–±–∞—Å)
    bass = 0.1 * np.sin(2 * np.pi * 30 * t)
    
    # Heartbeat-—Ä–∏—Ç–º (–ø—É–ª—å—Å–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ ~1.2 —Å–µ–∫—É–Ω–¥—ã)
    heartbeat = np.zeros_like(t)
    for i in range(int(duration / 1.2)):
        start = int(i * 1.2 * fps)
        end = int((i * 1.2 + 0.1) * fps)
        if end < len(heartbeat):
            heartbeat[start:end] = 0.05 * np.exp(-np.linspace(0, 10, end - start))
    
    # –î—ã—Ö–∞–Ω–∏–µ (6-—Å–µ–∫—É–Ω–¥–Ω—ã–π —Ü–∏–∫–ª)
    breath = 0.05 * np.sin(2 * np.pi * t / 6.0)
    
    audio = bass + heartbeat + breath
    audio = audio / np.max(np.abs(audio)) * 0.3  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    
    return audio


# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def process_neuro_video(
    input_path: str,
    output_path: str = "neuro_processed.mp4",
    gif_path: Optional[str] = "neuro_teaser.gif",
    gif_duration: float = 5.0,
    fps: int = 24,
    add_audio: bool = True
) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ —Å–æ –≤—Å–µ–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏ Neuro.
    """
    print(f"üé¨ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ: {input_path}")
    
    if not os.path.exists(input_path):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_path}")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–∏–¥–µ–æ
    clip = VideoFileClip(input_path)
    original_duration = clip.duration
    
    print(f"üìä –ò—Å—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ: {original_duration:.2f} —Å–µ–∫, {clip.fps} fps, {clip.size}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç—ã –∫ –∫–∞–∂–¥–æ–º—É –∫–∞–¥—Ä—É
    print("‚ú® –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤...")
    processed_clip = clip.fl(lambda gf, t: process_frame(gf(t), t))
    
    # –î–æ–±–∞–≤–ª—è–µ–º fade-in/fade-out –¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏
    processed_clip = processed_clip.fx(fadein, 0.5)
    processed_clip = processed_clip.fx(fadeout, 1.0)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞—É–¥–∏–æ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    if add_audio and clip.audio is None:
        print("üîä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ—Ñ–æ–Ω–∞...")
        from scipy.io import wavfile
        import tempfile
        
        audio_data = generate_breath_audio(processed_clip.duration)
        temp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        wavfile.write(temp_audio.name, 44100, (audio_data * 32767).astype(np.int16))
        temp_audio.close()
        
        audio_clip = AudioFileClip(temp_audio.name)
        processed_clip = processed_clip.set_audio(audio_clip)
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        def cleanup():
            os.unlink(temp_audio.name)
            audio_clip.close()
        processed_clip = processed_clip.on_close(cleanup)
    
    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –≤–∏–¥–µ–æ
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {output_path}")
    processed_clip.write_videofile(
        output_path,
        fps=fps,
        codec='libx264',
        preset='slow',
        bitrate='5000k',
        audio_codec='aac',
        audio_bitrate='192k'
    )
    
    # –°–æ–∑–¥–∞—ë–º GIF-—Ç–∏–∑–µ—Ä (–ø–µ—Ä–≤—ã–µ N —Å–µ–∫—É–Ω–¥)
    if gif_path:
        teaser_duration = min(gif_duration, processed_clip.duration)
        print(f"üéûÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ GIF-—Ç–∏–∑–µ—Ä–∞ ({teaser_duration:.1f} —Å–µ–∫): {gif_path}")
        teaser = processed_clip.subclip(0, teaser_duration)
        teaser.write_gif(
            gif_path,
            fps=15,
            program='ffmpeg',
            opt='optimizeplus'
        )
        teaser.close()
    
    # –û—á–∏—Å—Ç–∫–∞
    processed_clip.close()
    clip.close()
    
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")


# ============================================================================
# –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ê: –û–ë–†–ê–ë–û–¢–ö–ê –ò–ó –°–ö–†–ò–ù–®–û–¢–û–í
# ============================================================================

def process_from_screenshots(
    screenshots_dir: str,
    output_path: str = "neuro_from_screenshots.mp4",
    fps: int = 2
) -> None:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –≤–∏–¥–µ–æ –∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –≤ processed/ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —ç—Ñ—Ñ–µ–∫—Ç–æ–≤.
    """
    screenshots_path = Path(screenshots_dir)
    if not screenshots_path.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {screenshots_dir}")
        return
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ PNG —Ñ–∞–π–ª—ã
    image_files = sorted(screenshots_path.glob("*.png"))
    if not image_files:
        print(f"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {screenshots_dir}")
        return
    
    print(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    clips = []
    for i, img_path in enumerate(image_files):
        frame = np.array(Image.open(img_path).convert('RGB'))
        t = i / fps  # –í—Ä–µ–º—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
        processed_frame = process_frame(frame, t)
        
        # –°–æ–∑–¥–∞—ë–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–ª–∏–ø –∏–∑ –∫–∞–¥—Ä–∞
        img_clip = ImageSequenceClip([processed_frame], fps=fps)
        clips.append(img_clip)
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤ –æ–¥–Ω–æ –≤–∏–¥–µ–æ
    final_clip = concatenate_videoclips(clips, method="compose")
    
    # –î–æ–±–∞–≤–ª—è–µ–º fade
    final_clip = final_clip.fx(fadein, 0.5)
    final_clip = final_clip.fx(fadeout, 1.0)
    
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {output_path}")
    final_clip.write_videofile(
        output_path,
        fps=fps,
        codec='libx264',
        preset='slow',
        bitrate='5000k'
    )
    
    final_clip.close()
    for clip in clips:
        clip.close()
    
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")


# ============================================================================
# CLI
# ============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Neuro Video Processor")
    parser.add_argument("input", nargs="?", default="–ü—Ä–æ–º—Ç_–¥–ª—è_–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏_Neuro.mp4",
                       help="–í—Ö–æ–¥–Ω–æ–π –≤–∏–¥–µ–æ—Ñ–∞–π–ª")
    parser.add_argument("-o", "--output", default="neuro_processed.mp4",
                       help="–í—ã—Ö–æ–¥–Ω–æ–π –≤–∏–¥–µ–æ—Ñ–∞–π–ª")
    parser.add_argument("-g", "--gif", default="neuro_teaser.gif",
                       help="–ü—É—Ç—å –¥–ª—è GIF-—Ç–∏–∑–µ—Ä–∞")
    parser.add_argument("--no-gif", action="store_true",
                       help="–ù–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å GIF")
    parser.add_argument("--no-audio", action="store_true",
                       help="–ù–µ –¥–æ–±–∞–≤–ª—è—Ç—å –∞—É–¥–∏–æ")
    parser.add_argument("--from-screenshots", action="store_true",
                       help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –≤ processed/")
    parser.add_argument("--screenshots-dir", default="processed",
                       help="–ü–∞–ø–∫–∞ —Å–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏ (–µ—Å–ª–∏ --from-screenshots)")
    
    args = parser.parse_args()
    
    if args.from_screenshots:
        process_from_screenshots(
            args.screenshots_dir,
            args.output
        )
    else:
        process_neuro_video(
            args.input,
            args.output,
            None if args.no_gif else args.gif,
            add_audio=not args.no_audio
        )

